{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading in Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = pd.read_csv(\"../input/fake-news-detection/data.csv\")\ndf_real = pd.read_csv(\"../input/news-articles/fake_news_set.csv\")\ndf_fake = pd.read_csv(\"../input/news-articles/real_news_set.csv\")\ndf = pd.concat([df_real, df_fake], axis=0).sample(frac=1).reset_index(drop=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing\nWe use a Keras Tokenizer to sequences of word indices. Then, we use GloVe word embeddings to map them to embedding vectors.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\ndf['body'].apply(lambda x: [item for item in x if item not in stop])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"max_features = 25000\nmaxlen = 1000\nembedding_size = 200\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(df['body']))\nX = tokenizer.texts_to_sequences(df['body'])\nX = pad_sequences(X, maxlen = maxlen)\ny = df['label']","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove200dtxt/glove.6B.200d.txt'\n\n\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":20,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  import sys\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Preparing the Embedding and CNN\nWe create an embedding layer using the embedding matrix as weights. From there, we build a 1D convnet to classify the news articles."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embedding_size, trainable=False, weights=[embedding_matrix])) # add weights\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(40, activation=\"relu\"))\nmodel.add(Dropout(0.7))\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=25, verbose=1, mode='auto', restore_best_weights=True)\n\n\nbatch_size = 32\nepochs = 30\nhistory = model.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks = [early])\n\n","execution_count":21,"outputs":[{"output_type":"stream","text":"Train on 14922 samples, validate on 3731 samples\nEpoch 1/30\n14922/14922 [==============================] - 52s 3ms/step - loss: 0.4774 - acc: 0.7567 - val_loss: 0.2019 - val_acc: 0.9233\nEpoch 2/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.2373 - acc: 0.9103 - val_loss: 0.1221 - val_acc: 0.9566\nEpoch 3/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.1602 - acc: 0.9418 - val_loss: 0.1138 - val_acc: 0.9585\nEpoch 4/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.1178 - acc: 0.9564 - val_loss: 0.0832 - val_acc: 0.9729\nEpoch 5/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0898 - acc: 0.9700 - val_loss: 0.0819 - val_acc: 0.9737\nEpoch 6/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0652 - acc: 0.9773 - val_loss: 0.0862 - val_acc: 0.9759\nEpoch 7/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0524 - acc: 0.9826 - val_loss: 0.1172 - val_acc: 0.9748\nEpoch 8/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0405 - acc: 0.9863 - val_loss: 0.0770 - val_acc: 0.9769\nEpoch 9/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0406 - acc: 0.9863 - val_loss: 0.0897 - val_acc: 0.9791\nEpoch 10/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0415 - acc: 0.9857 - val_loss: 0.0983 - val_acc: 0.9756\nEpoch 11/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0335 - acc: 0.9878 - val_loss: 0.1399 - val_acc: 0.9743\nEpoch 12/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0344 - acc: 0.9863 - val_loss: 0.1113 - val_acc: 0.9775\nEpoch 13/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0337 - acc: 0.9872 - val_loss: 0.1190 - val_acc: 0.9767\nEpoch 14/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0263 - acc: 0.9907 - val_loss: 0.1475 - val_acc: 0.9759\nEpoch 15/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0279 - acc: 0.9887 - val_loss: 0.1364 - val_acc: 0.9783\nEpoch 16/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0338 - acc: 0.9873 - val_loss: 0.1420 - val_acc: 0.9719\nEpoch 17/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0232 - acc: 0.9906 - val_loss: 0.1594 - val_acc: 0.9761\nEpoch 18/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0288 - acc: 0.9877 - val_loss: 0.1469 - val_acc: 0.9759\nEpoch 19/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0258 - acc: 0.9883 - val_loss: 0.1481 - val_acc: 0.9751\nEpoch 20/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0245 - acc: 0.9898 - val_loss: 0.1473 - val_acc: 0.9786\nEpoch 21/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0243 - acc: 0.9892 - val_loss: 0.1299 - val_acc: 0.9788\nEpoch 22/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0177 - acc: 0.9923 - val_loss: 0.1961 - val_acc: 0.9783\nEpoch 23/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0243 - acc: 0.9914 - val_loss: 0.1371 - val_acc: 0.9788\nEpoch 24/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0223 - acc: 0.9903 - val_loss: 0.1362 - val_acc: 0.9756\nEpoch 25/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0224 - acc: 0.9903 - val_loss: 0.1469 - val_acc: 0.9759\nEpoch 26/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0192 - acc: 0.9928 - val_loss: 0.1413 - val_acc: 0.9756\nEpoch 27/30\n14922/14922 [==============================] - 50s 3ms/step - loss: 0.0130 - acc: 0.9948 - val_loss: 0.2052 - val_acc: 0.9767\nEpoch 28/30\n10464/14922 [====================>.........] - ETA: 13s - loss: 0.0221 - acc: 0.9929","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-118948c0a14f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"We can compare the output to a CNN with no preset embedding weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embedding_size, trainable=False))\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(40, activation=\"relu\"))\nmodel.add(Dropout(0.7))\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=25, verbose=1, mode='auto', restore_best_weights=True)\n\n\nbatch_size = 32\nepochs = 30\nhistory = model.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks = [early])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}